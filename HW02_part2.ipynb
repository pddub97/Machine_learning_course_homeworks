{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUezHfp_DxSA"
   },
   "source": [
    "# Home Assignment No. 2: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ce4lF9ibDxSD"
   },
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vkeI1uqDxSF"
   },
   "source": [
    "# Task1. Bayesian methods (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T80dEeRVDxSH"
   },
   "source": [
    "For a dataset $D =(X,y) =\\{(x_i,y_i)\\}^m_{i=1}$, $x_i \\in \\mathbb{R}^d$, $y_i\\in\\mathbb{R}$ it is known,that \n",
    "$$y_i = w^T x_i + \\epsilon$$\n",
    "where $\\epsilon \\sim N(0,\\sigma^2)$, $w  \\sim N(0,\\alpha I)$ . Suppose that $X^T X =I$, where $I$ is the identity matrix. Derive MAP estimation for $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aqGGfQdDxSI"
   },
   "source": [
    "### Your solution\n",
    "Apriory probability ow $w$:\n",
    "$$\n",
    "P(w|0,\\alpha I) = \\frac{1}{\\sqrt{2\\pi\\alpha I}} \\exp{\\frac{-w^2}{2\\pi\\alpha I}} =\n",
    "$$\n",
    "A-posteriori probability:\n",
    "$$\n",
    " P(w|D)= \\frac{P(w|0,\\alpha I) P(D|w)}{P(D)}\n",
    "$$\n",
    "Where \n",
    "$$\n",
    "P(D|w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\frac{-(y-w^Tx)^2}{2\\pi\\sigma^2}}\n",
    "$$\n",
    "Maximum a-posteriori (MAP):\n",
    "$$\n",
    "w_{MAP} = argmax_w  log(P(w|D))=argmax_w log(P(w))+log(P(D|w))+log(P(D))\n",
    "$$\n",
    "Since $P(D)$ is a constant and does not affect optimization:\n",
    "$$\n",
    "w_{MAP}= argmax_w\\big(log(P(w)+log(P(D|w))\\big) = argmax_w\\bigg( log(\\frac{1}{\\sqrt{2\\pi\\alpha I}} \\exp{\\frac{-w^2}{2\\pi\\alpha I}})+log(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\frac{-(y-w^Tx)^2}{2\\pi\\sigma^2}}))\\bigg)\n",
    "$$\n",
    "Eliminate all the constants, change signs and, therefore, maximize to minimize and simplify the expression:\n",
    "$$\n",
    "w_{MAP}= argmin_w\\big(  \\frac{-||w||^2}{\\alpha} +\\frac{1}{\\sigma^2}\\sum_{i=1}^{m}(y_i-w^Tx_i) ^2 \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pn6W9rhoDxSJ"
   },
   "source": [
    "# Task 2. Gaussian Processes 1 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Sk_WNJiDxSL"
   },
   "source": [
    "Let $\\sigma_n(\\mathbf{x}_*)$ be a predictive variance at point $\\mathbf{x}_*$ of a Gaussian Process $f_n$ with zero mean and covariance $k(\\cdot,\\cdot)$ that was built using first $n$ training points.\n",
    "Prove that for $\\forall \\mathbf{x}_*$ it holds\n",
    "$$\n",
    "    \\sigma_{n}(\\mathbf{x}_*) \\leq \\sigma_{n-1}(\\mathbf{x}_*).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GF77gvmtDxSM"
   },
   "source": [
    "### Your solution\n",
    "Predicted variance of a Gaussian Process can be represented as follows:\n",
    "$$\n",
    "\\sigma_{n}(\\mathbf{x}_*) = k(x_*, x_*) - k_*^T(K_n+\\sigma^2I_n)^{-1} k_*\n",
    "$$\n",
    "Then the inequality can be rewritten the following way:\n",
    "$$\n",
    "     k_*^T(K_n+\\sigma^2I_n)^{-1} k_* \\geq k_{n-1}(x_*)^T(K_{n-1}+\\sigma^2I_{n-1})^{-1} k_{n-1} (x_*)\n",
    "$$\n",
    "Matrix $K_n+\\sigma^2I_n$ can be written the following way:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "K_{n-1}+\\sigma^2I_{n-1} & k_{n-1}(x_n) \\\\\n",
    "k_{n-1}(x_n)^T & k(x_n,x_n)+ \\sigma^2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Here $k_{n-1}(x_n)$ is a vector of covariences:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "k(x_1,x_n) \\\\\n",
    "\\dots\\\\\n",
    "k(x_{n-1},x_n)\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Inversed matrix $(K_n+\\sigma^2I_n)^{-1}$ can be found by the formula of the inversed matrix from block matrix:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "A & B \\\\\n",
    "C & D\n",
    "\\end{pmatrix}^{-1}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} & -A^{-1}B(D-CA^{-1}B)^{-1} \\\\\n",
    "-(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Lets substitute (denoting $G =(D-CA^{-1}B)^{-1} $) $B, C, D$ with the corresponding blocks and multimply by square of $k_* $:\n",
    "$$\n",
    "k_*^T\n",
    "\\begin{pmatrix}\n",
    "A^{-1}+A^{-1}k_{n-1}(x_n)Gk_{n-1}(x_n)^TA^{-1} & -A^{-1}k_{n-1}(x_n)G \\\\\n",
    "-Gk_{n-1}(x_n)^TA^{-1} & G\n",
    "\\end{pmatrix}\n",
    "k_*\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "k_{n-1}(x_*) \\\\\n",
    "k_{n}(x_*)\\\\\n",
    "\\end{pmatrix}^T\n",
    "\\begin{pmatrix}\n",
    "A^{-1}+A^{-1}k_{n-1}(x_n)Gk_{n-1}(x_n)^TA^{-1} & -A^{-1}k_{n-1}(x_n)G \\\\\n",
    "-Gk_{n-1}(x_n)^TA^{-1} & G\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "k_{n-1}(x_*) \\\\\n",
    "k_{n}(x_*)\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Now lets multimply and get the following expression:\n",
    "$$ k_{n-1}^T(x_*)A^{-1}k_{n-1}(x_*)+k_{n-1}^T(x_*)A^{-1}k_{n-1}(x_n)Gk_{n-1}^T(x_n)A^{-1})k_{n-1}(x_*)+k_{n}(x_*)(-Gk_{n-1}(x_n)^TA^{-1})k_{n-1}(x_*)+k_{n-1}(x_*) (-A^{-1}k_{n-1}(x_n)G)k_{n}(x_*)+k_{n}(x_*)Gk_{n}(x_*)$$\n",
    "Lets  simplify the expression:\n",
    "$$k_{n-1}^T(x_*)A^{-1}k_{n-1}(x_*)+G(k_{n-1}^T(x_*)A^{-1}k_{n-1}(x_n)-k_{n}(x_*))^2 $$\n",
    "The right term is always positive, the left is: $$ k_{n-1}(x_*)^T(K_{n-1}+\\sigma^2I_{n-1})^{-1} k_{n-1} (x_*)$$ Which is lower then the initial expression by the positive right term, that means, that $$ k_*^T(K_n+\\sigma^2I_n)^{-1} k_* \\geq k_{n-1}(x_*)^T(K_{n-1}+\\sigma^2I_{n-1})^{-1} k_{n-1} (x_*)$$ Q.E.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4kxRa0DDxSN"
   },
   "source": [
    "# Task 3. Gaussian Processes 2 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9S2_ZiC7DxSP"
   },
   "source": [
    "Consider you have gaussian distribution on $R$ with zero mean and differentiable by arguments covariation funtion $k(x, \\tilde{x})$.Get an expression for the correlation between the implementation of a Gaussian process  $y(x) âˆ¼ GP (0, k(x, x ^{\\prime}))$ and its derivative $\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjQUIVBrDxSQ"
   },
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7ZaldlwDxSR"
   },
   "source": [
    "# Task 4. Kernel theory (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M2sCzHwDDxST"
   },
   "source": [
    "Let $K(x, x'):\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}$ be a PDS kernel,\n",
    "and $\\phi\\colon \\mathcal{X} \\to \\mathcal{H}$ its <b>unknown </b> feature mapping. For $x,x'\\in\\mathcal{X}$ derive the formula for the **distance** between $\n",
    "\\phi(x)$ and $\\phi(x')$ in $\\mathcal{H}$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAD_0hAHDxSU"
   },
   "source": [
    "### Your solution\n",
    "Distance between two points squared is a squared norm of their difference:\n",
    "$$\n",
    "||\\phi(x)-\\phi(x')||^2=(\\phi(x)-\\phi(x'),\\phi(x)-\\phi(x')) = (\\phi(x),\\phi(x)) +(\\phi(x'),\\phi(x'))-2(\\phi(x),\\phi(x'))\n",
    "$$\n",
    "Then the distance:\n",
    "$$\n",
    "\\sqrt{(\\phi(x),\\phi(x))+(\\phi(x'),\\phi(x'))-2(\\phi(x),\\phi(x')} = \\sqrt{(K(x,x)+K(x',x')-2K(x,x')}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fAJG78M-DxSV"
   },
   "source": [
    "# Task 5. Naive Gradient Boosting Regression (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfqoctefDxSX"
   },
   "source": [
    "You are given a regression dataset, consisting of 5 samples with 1-dimensional feature vector $X$ and scalar target vector $y \\in \\mathbb{R}$:\n",
    "\n",
    "|  x   |  y   | \n",
    "|:----:|:----:| \n",
    "|  10  |  1   | \n",
    "|  32  |  9   | \n",
    "|  46  |  13  | \n",
    "|  54  |  16  | \n",
    "|  63  |  23  | \n",
    "\n",
    "In this task you are asked to implement **3 steps of Gradient Boosting Regression** with decision tree stumps as the learners $h_0, h_1, h_2$. \n",
    "\n",
    "In order to complete this task:\n",
    "1. Refer to the slides on naive boosting for regression in **Lecture 8**.\n",
    "2. Assume that the initial model $f_0$ is the mean of the target vector $y$\n",
    "3. According to the algorithm on the boosting approach for regression from **1.**, compute the residuals\n",
    "4. Manually, find a suitable split among the $x_i$ for each decision tree weak model $h_t(X)$, which minimizes the loss function:\n",
    "\n",
    "$$L_{\\text{split_i}} = \\frac{\\text{Var}_{left\\_split}*N_{1} + \\text{Var}_{right\\_split}*N_{2}}{N_{1}+N_{2}}$$\n",
    "\n",
    "where  $\\text{Var}$ is the variance of the values contained in each leaf, $N_1$ is the number of target values $y$ in the left leaf, $N_{2}$ - in the right leaf\n",
    "\n",
    "5. Perform the Gradient Boosting step on the ensemble model $f_t$ with the resulting decision tree stump predictions (assume that the learning rate $lr=1.0$).\n",
    "\n",
    "**Note on Decision Tree Stumps:** A decision tree stump is a decision tree, which consists only of the root and its immediate leaves. In case of this task, at each iteration you are asked to consider 5 different variants of the decision tree stumps $h_t^i$ - one variant for each of the split candidates $x_i$. You should choose the variant that minimizes the loss written above. The two leaves of the tree are formed according to the rule:\n",
    "\n",
    "```python\n",
    "if x_i < split:\n",
    "    target_value -> left leaf\n",
    "elif x_i >= split:\n",
    "    target_value -> right leaf\n",
    "```\n",
    "**HINT:** Think about what should be `target_value` equal to in case of Gradient Boosting Regression.\n",
    "\n",
    "The prediction of decision tree stump $h_t(x_i)$ is the mean of the values of the according leaf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ug4fgxYDxSY"
   },
   "source": [
    "**The task**:\n",
    "\n",
    "* Fill in the table - round the values of table up to the second digit after decimal point:\n",
    "\n",
    "\n",
    "|   x  |   y  |$f_0$|$$y - f_0$$|$L$|$h_0$|$f_1$|$$y-f_1$$|$L$|$h_1$|$f_2$|$$y - f_2$$|$L$|$h_2$|$F_3$|\n",
    "|------|------|-----|-----------|---|-----|-----|---------|---|-----|-----|-----------|---|-----|-----|\n",
    "|  10  |  1   |  12.40  |    -11.40      | NaN |  -7.40  |  5  |    -4    | NaN |  -1.42  |  3.58  |    -2.58      | NaN |  -2.58  |  1  | \n",
    "|  32  |  9   |  12.40  |    -3.40      | 20.95 |  -7.40  |  5  |    4    | 12.93 |  -1.42  |  3.58  |    5.42      | 7.24 |  0.65  |  4.23  |\n",
    "|  46  |  13  |  12.40  |    0.60      | 16.93 |  4.93  |  17.33  |    -4.33    | 16.93 |  -1.42  |  15.92  |    -2.92      | 7.57 |  0.65  |  16.56  |\n",
    "|  54  |  16  |  12.40  |    3.60      | 19.83 |  4.93  |  17.33  |    -1.33    | 13.80 |  -1.42  |  15.92  |    0.08      | 8.90 |  0.65  |  16.56  |\n",
    "|  63  |  23  |  12.40  |    10.6      | 25.35 |  4.93  |  17.33  |    5.67    | 8.91 |  5.67  |  23  |    0      | 8.91 |  0.65  |  23.65  |\n",
    "\n",
    "\n",
    "where $L$ is the loss, calculated by the formula for decision tree stumps above, for each of the 5 split variants of the decision tree stump at each iteration\n",
    "* Write down the splits (the feature values) you have found for each of the tree stumps\n",
    "\n",
    "* Insert the predictions of the full ensemble model and the split values, you have achieved after 3 iterations into the plotting cell below (**COPY AND PASTE** the last column from the table above and the splits list to the plotting cell below, instead of **#your solution**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCe9fecPDxSZ"
   },
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tPJf1QAvDxSf"
   },
   "outputs": [],
   "source": [
    "def plot_tree(x,F,stumps):\n",
    "    x_range = np.arange(np.min(x), np.max(x)+1)\n",
    "    x_r = []\n",
    "    f_r = []\n",
    "    stmps = [0] + stumps + [np.inf]\n",
    "    for st in range(1,len(stmps)):\n",
    "        x_r.extend([list(group) for k, group in groupby(x_range, lambda x: x<stmps[st] and x>=stmps[st-1]) if k])\n",
    "        f_r.append([f_i for f_i,x_ii in zip(F,x) if x_ii<stmps[st] and x_ii>=stmps[st-1]])\n",
    "    F_to_plot = []\n",
    "    for ft in range(len(f_r)):\n",
    "        #assert len(f_r) == len(x_r)\n",
    "        if len(f_r[ft]) == 1:\n",
    "            F_to_plot.extend([f_r[ft][0]]*len(x_r[ft]))\n",
    "        elif len(f_r[ft]) > 1:\n",
    "            F_to_plot.extend([mean(f_r[ft])]*len(x_r[ft]))\n",
    "    return F_to_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1IptGc-DxSk"
   },
   "source": [
    "## PLOTTING CELL##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "CA8iY9WUDxSl",
    "outputId": "6c8f06aa-9654-4e5a-a152-0e50acbc44a9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAFpCAYAAABTfxa9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [10,32,46,54,63]\n",
    "y = [1, 9, 13, 16, 23]\n",
    "\n",
    "#note that the order of F(x_i) should be corresponding to the order of x_i in the table\n",
    "f0 = mean(y)\n",
    "############ INSERT YOUR SOLUTION HERE###############\n",
    "y_f0 = [y[i]-f0 for i in range(len(y))]\n",
    "\n",
    "#Function to split sample by some split value\n",
    "def tree_split(x, y, split):\n",
    "  x_left = []\n",
    "  x_right = []\n",
    "  y_left = []\n",
    "  y_right = []\n",
    "  for i in range(len(x)):\n",
    "    if x[i] < split:\n",
    "      x_left += [x[i]]\n",
    "      y_left += [y[i]]\n",
    "    elif x[i] >= split:\n",
    "      x_right += [x[i]]\n",
    "      y_right += [y[i]]\n",
    "  result = [x_left, x_right, y_left, y_right]\n",
    "  return result\n",
    "\n",
    "#Function to find a split resulting in the smallest L value\n",
    "def find_best_split(x, y):\n",
    "  L = []\n",
    "  for k in x:\n",
    "    split = k\n",
    "    leafs = tree_split(x, y, split)\n",
    "    if len(leafs[2]) ==0:\n",
    "      L += ['NaN']\n",
    "    else:\n",
    "      L += [(np.var(leafs[2] )*len(leafs[2])+np.var(leafs[3])*len(leafs[3]))/(len(leafs[2])+len(leafs[3]))]\n",
    "  split = x[np.argmin(L[1:])+1]\n",
    "  leafs = tree_split(x, y, split)\n",
    "  return split, leafs, L\n",
    "\n",
    "# Function to find value of h and f in different leafes\n",
    "def f(leafs, y, f0):\n",
    "  hl = mean(leafs[2])\n",
    "  hr = mean(leafs[3])\n",
    "  \n",
    "  h  =[]\n",
    "  for i in range(len(y)):\n",
    "    if type(f0) !=list:\n",
    "      f0 = [f0 for i in range(len(y))]\n",
    "    if i < len(leafs[2]):\n",
    "      h += [hl]\n",
    "    else:\n",
    "      h += [hr]\n",
    "  f =[]\n",
    "  for i in range(len(y)):\n",
    "    f += [f0[i]+h[i]]\n",
    "\n",
    "  return h, f\n",
    "\n",
    "# Calling the functions in the iterative order\n",
    "split1, leafs1, L1 = find_best_split(x, y_f0)\n",
    "\n",
    "h0, f1 = f(leafs1, y, f0)\n",
    "y_f1 = [y[i]-f1[i] for i in range(len(y))]\n",
    "\n",
    "split2, leafs2, L2 = find_best_split(x, y_f1)\n",
    "\n",
    "h1, f2 = f(leafs2, y, f1)\n",
    "\n",
    "y_f2 = [y[i]-f2[i] for i in range(len(y))]\n",
    "\n",
    "split3, leafs3, L3 = find_best_split(x, y_f2)\n",
    "\n",
    "h2, f3 = f(leafs3, y, f2)\n",
    "\n",
    "F3 = f3\n",
    "\n",
    "splits = [split1, split2, split3]\n",
    "\n",
    "boosted_F_plot = plot_tree(x, F3, stumps = list(np.sort(splits)))\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(x,y, label = 'original')\n",
    "ax.scatter(x, F3, label = 'predicted')\n",
    "x_range = np.arange(np.min(x), np.max(x)+1)\n",
    "ax.plot(x_range,boosted_F_plot,'--', linewidth=2, label = 'composite function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izrx5dwaDxSp"
   },
   "source": [
    "# Task 6. AdaBoost (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBChGRH0DxSq"
   },
   "source": [
    "For each of the following cases,explain how AdaBoost, as given in **Lecture 7**, will treat a weak hypothesis $h_t$ with weighted error $N_t(h_t , w_t )$. Also, in each case, explain why this behavior takes place.\n",
    "1. $N_t = \\frac{1}{2}$\n",
    "2. $N_t > \\frac{1}{2}$\n",
    "3. $N_t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2VRN32xDxSs"
   },
   "source": [
    "### Your solution\n",
    "Weight of each estimator in AdaBoost is calculated with help of the following formula:\n",
    "$$\n",
    "\\alpha = \\frac{1}{2}log( \\frac{1-N_t}{N_t} )\\space (1)\n",
    "$$\n",
    "From which we can easily interpretate influence of different values of $N_t$.\n",
    "\n",
    "1) $N_t = \\frac{1}{2}$ if we put this value in (1) we get:\n",
    "$$\n",
    "\\alpha = \\frac{1}{2}log1 = 0\n",
    "$$\n",
    "Zero wheight of an estimator $h_t$ means, that it does not participate in voting and have no influence on the final result.\n",
    "\n",
    "2) $N_t > \\frac{1}{2}$ this value in $\\frac{1-N_t}{N_t}$ gives a value between 0 and 1 if we put it in (1) we get $\\alpha <  0$ since logarithm is negative, when its argument between 0 and 1. That means, that the weight of a classifier is negative and it will reverse the sign, that the classifier gives.\n",
    "\n",
    "3)$N_t=0$ if we put this value in (1), we get:\n",
    "$$\n",
    "\\alpha =  \\frac{1}{2}log( \\frac{1-0}{0} ) = +\\infty\n",
    "$$\n",
    "If the classifier has an error equal to 0, it gives a weight going to infinity, and this estimator outperformes any others. That means, that we can rely only on this classifier, because it is an ideal one. But it is not a practical kind of situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9lpbj48w6NI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW02-part2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
