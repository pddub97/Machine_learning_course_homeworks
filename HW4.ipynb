{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq0k7nB8sNz8",
        "colab_type": "text"
      },
      "source": [
        "# Home Assignment No. 4: Part 1 (Practice)\n",
        "To solve this task, you will write some code to try random **recurrent neural networks** in action. This part of the assignment is a simple practice with pytorch.\n",
        "* You are **HIGHLY RECOMMENDED** to read relevant documentation, e.g. for [python](https://docs.python.org/3/), [numpy](https://docs.scipy.org/doc/numpy/reference/), [matlpotlib](https://matplotlib.org/) and [pytorch](http://pytorch.org). Also remember that seminars, lecture slides, [Google](http://google.com) and [StackOverflow](https://stackoverflow.com/) are your close friends during this course (and, probably, whole life?).\n",
        "\n",
        "* In some problems you are asked to provide short discussion of the results. In these cases you have to create **MARKDOWN** cell with your comments right after the corresponding code cell.\n",
        "\n",
        "* For every separate (sub)problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**. So make sure that you did everything required in the task\n",
        "\n",
        "* Your **SOLUTION** notebook **MUST BE REPRODUCIBLE**, i.e. if the reviewer decides to execute all, after all the computation he will obtain exactly the same solution (with all the corresponding plots) as in your uploaded notebook.\n",
        "\n",
        "* Your code must be clear to the reviewer. For this purpose, try to include neccessary comments inside the code. But remember: **GOOD CODE MUST BE SELF-EXPLANATORY** without any additional comments.\n",
        "\n",
        "## Recurrent Neural Networks\n",
        "\n",
        "During the seminar, we worked on building an intution for RNNs. We saw that they are very powerful techniques and compared to ARMA methods, where some assumptions must hold to apply the model, RNNs can be applied on sequential data as blackboxes and lead to good results.\n",
        "\n",
        "Recurrent Neural Networks (RNN) can read inputs $x^{\\langle t \\rangle}$ one by one. Through the hidden layer activations, they can \"remember\" some information. The RNN can pass it to the next step, in the case of the unidirectional RNN, or to previous and future steps in the bidirectional RNN.\n",
        "\n",
        "Before continuing, we should define some notation:\n",
        "* $[l]$ represents an object associated with the $l^{th}$ layer. \n",
        "* Whereas $(i)$ denotes an object associated with the $i^{th}$ example. \n",
        "\n",
        "- Superscript $\\langle t \\rangle$ denotes an object at the $t^{th}$ time-step. \n",
        "    \n",
        "- **Sub**script $i$ denotes the $i^{th}$ entry of a vector.\n",
        "\n",
        "Example:  \n",
        "- $a^{(2)[3]<4>}_5$ denotes the activation of the 2nd training example (2), 3rd layer [3], 4th time step <4>, and 5th entry in the vector.\n",
        "\n",
        "We first define some functions that will be helpful for us in the next steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L69jWPgZsNz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q8IjYLPsN0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    e_x = torch.exp(x - torch.max(x))\n",
        "    return e_x / e_x.sum(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s0sbmX5sN0J",
        "colab_type": "text"
      },
      "source": [
        "The RNN can be intuitively understood as following"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb0CHrgAsN0K",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"RNN_1.gif\" style=\"width:500;height:300px;\">\n",
        "<caption><center>Figure 1: The Recurrent Neural Network</center></caption>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CelWULsTsN0L",
        "colab_type": "text"
      },
      "source": [
        "Let's discuss this implementation more in detail.\n",
        "## Inputs and outputs of a RNN\n",
        "We start with the two input vectors of the RNN, the input vector and the hidden state.\n",
        "### Input $x$\n",
        "The number of units is represented as $n_x$. It can be understood as the number of units in a single timestep of a single training example.\n",
        "\n",
        "A timestep in a single input example is represented as $x^{(i) \\langle t \\rangle }$. The timesteps are indexed as $t$. Normally, a single training example $x^{(i)}$ will consist of multiple time steps $T_x$.\n",
        "\n",
        "The training examples are grouped into so-called mini batches $x$ of size $m$. Thus, the shape of a mini batch is $(n_x,m,T_x)$. $x$ is thus a 3-dimension tensor and this will be our input into the RNN.\n",
        "\n",
        "As we will be working with mini-batches of training examples, we will need a 2D slide $x^{\\langle t \\rangle}$ of the same shape of our mini batch, $(n_x, m)$. In our implementation, this will be the variable `xt`.\n",
        "### Hidden State $a$\n",
        "The second input required in a RNN is the hidden state $a^{\\langle t \\rangle}$, an activation function. The hidden state is passed from one step to the next. It shares similarities with our input vector $x$. \n",
        "\n",
        "For example, it has a length $n_{a}$, and if we include a mini-batch of size $m$, the shape of the mini-batch will be $(n_{a},m)$. The hidden state also makes use of the same index $t$ to iterate and on each loop it uses a 2D slice $a^{\\langle t \\rangle}$ of shape $(n_{a}, m)$.\n",
        "\n",
        "In our implementation, we will have two variables for the hidden state. They will be called `a_prev` and `a_next`.\n",
        "Similarly, on each step, we produce an output vector.\n",
        "### Output $\\hat{y}$\n",
        "The ouput $\\hat{y}$ is also a 3D tensor. Its shape is $(n_{y}, m, T_{y})$, where $n_{y}$ represents the number of units in the vector representing the prediction. As it is the case with the input, $m$ reflects the number of examples in the mini batch and $T_{y}$ the number of steps in the prediction. In our implementation, we will be defining this variable as `y_pred`.\n",
        "\n",
        "If we go more in depth and look at a single time step $t$, we have a 2D slice $\\hat{y}^{\\langle t \\rangle}$ with shape $(n_{y}, m)$. Similarly, the variable in our code for this 2D slice will be called `yt_pred`.\n",
        "## The RNN cell\n",
        "In the animation above, we see that the RNN model consists basically of looping over a single cell over the index $t$. As we outlined above, we take the input consisting of two vectors, the current input $x^{\\langle t \\rangle}$, and the previous hidden state $a^{\\langle t - 1\\rangle}$. \n",
        "At the same time, we will be passing to the next RNN cell two outputs. They are the hidden state  $a^{\\langle t \\rangle}$ and the prediction $\\hat{y}^{\\langle t \\rangle}$.\n",
        "Let's start by implementing the RNN cell!\n",
        "## Task 1. Simple Cell (4 points)\n",
        "In this task, you will have 4 individual exercises:\n",
        "* **(1 pt.)** First, start by implementing the hidden state $a$ with the tanh activation function. It is defined as \n",
        "$$\n",
        "\\begin{align}\n",
        "a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a).\n",
        "\\end{align}\n",
        "$$\n",
        "* **(1 pt.)** Second, use the obtained result in the first task to compute the prediction \n",
        "$$\n",
        "\\begin{align}\n",
        "\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y).\n",
        "\\end{align}\n",
        "$$\n",
        "* **(1 pt.)** Third, proceceed to store the tuple $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ in a `accumulator` variable.\n",
        "* **(1 pt.)** Fourth return `accumulator` and $a^{\\langle t \\rangle}$ , $\\hat{y}^{\\langle t \\rangle}$.\n",
        "The function to implement consists of the following arguments:\n",
        "* `xt` - This is the input data at timestep $t$, a pytorch tensor of shape $(n_x, m)$\n",
        "* `a_prev` - Hidden state at timestep \"t-1\", a pytorch tensor of shape $(n_a, m)$\n",
        "* `parameters` - This is a python dictionary. It consists of five keys:\n",
        "* - Wax - The weight matrix multiplying the input $x$ with shape $(n_a, n_x)$\n",
        "* - Waa - The meight matrix multiplying the hidden state $a$ with shape $(n_a, n_a)$\n",
        "* - Wya - The weight matrix for the hidden-state to the output with shape $(n_y, n_a)$\n",
        "* - ba -  Represents the bias with shape $(n_a, 1)$\n",
        "* - by -  Similarly, this is the bias for the hidden-state to the output. It has a shape $(n_y, 1)$\n",
        "    \n",
        "The function must return the following variables:\n",
        "* `a_next` - This is the next hidden state with shape $(n_a, m)$\n",
        "* `yt_pred` - It is the prediction at timestep $t$ with shape $(n_y, m)$\n",
        "* `accumulator` - A tuple of values required for the backward pass. The tuple must contain (`a_next`, `a_prev`, `xt`, `parameters`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGGLxVECsN0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_cell_forward(xt, a_prev, parameters):    \n",
        "\n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "    \n",
        "    ### WRITE SOLUTION HERE ###\n",
        "    a_next = np.tanh(Waa@a_prev+Wax@xt+ba)\n",
        "    yt_pred = softmax(torch.tensor(Wya@a_next+by))\n",
        "    accumulator = (a_next, a_prev, xt, parameters)\n",
        "    return accumulator, a_next, yt_pred\n",
        "    \n",
        "    ### WRITE SOLUTION HERE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz9DxZFgsN0O",
        "colab_type": "text"
      },
      "source": [
        "Let's put our implementation to test! For this, we will define a time series with seasonality using the functions below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feNAfloIsN0P",
        "colab_type": "text"
      },
      "source": [
        "Let's also define some variables to control our series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxLmz7HtsN0R",
        "colab_type": "text"
      },
      "source": [
        "## The RNN Forward Pass\n",
        "In the previous task, we computed one iteration of a RNN cells. However, the RNN consists of a repetition of RNN cells. For each step in index $t$, we have to use the RNN cell again. \n",
        "\n",
        "Let's not forget that on each step, the RNN cell takes two inputs, the current input data $x^{\\langle t \\rangle}$ and the hidden state from the previous cell, $a^{\\langle t-1 \\rangle}$. The output, therefore, has to be two vectors, the prediction ($y^{\\langle t \\rangle}$) and the hidden state ($a^{\\langle t \\rangle}$). If you remember our previous task, we defined a set of weights and biases named $(W_{aa}, b_{a}, W_{ax}, b_{x})$. They will be used on each time step as well.\n",
        "\n",
        "Let's implement a forward pass for our RNN!\n",
        "\n",
        "## Task 2. Go Forward (1+1+1+1+1=5 points)\n",
        "This task will consist of 5 simple exercises. They are the following:\n",
        "* **(1 pt.)** First, initializing a 3D array $a$ with zeros. It must have the shape $(n_{a}, m, T_{x})$. This array must store all the hidden states computed by our RNN\n",
        "* **(1 pt.)** Second, similarly, we will need a 3D array to store the predictions. The array $\\hat{y}$ must have the same shape as the array $a$, this means a shape $(n_{y}, m, T_{x})$\n",
        "* **(1 pt.)** Third, please create the 2D hidden state `a_next`. For this, you must set it equal to $a_{0}$, the initial hidden state.\n",
        "* **(1 pt.)** Fourth, you must iterate through each step $t$. In each loop, the RNN must do the following:\n",
        "* - Retrieve $x^{\\langle t \\rangle}$ with shape $(n_{x}, m)$. This is the 2D slide of $x$ at a time step $t$\n",
        "* - Call the function `rnn_cell_forward` to update:\n",
        "* -- the hidden state `a_next` with shape $(n_{a}, m)$\n",
        "* -- the prediction $\\hat{y}^{\\langle t \\rangle}$\n",
        "* -- and the accumulator\n",
        "* - At the $t^{th}$ position:\n",
        "* -- Store the 2D hidden state in the 3D tensor $a$ with shape $(n_{a}, m, T_{x})$\n",
        "* -- Store the 2D prediction `yt_pred` with shape $(n_{y}, m)$ in the 3D tensor $\\hat{y}_{pred}$ with shape $(n_{y}, m, T_x)$\n",
        "* - Append the accumulator to a list of accumulators\n",
        "* - Store the input data `yt_pred` and the list of accumulator into a tuple `acc`\n",
        "* **(1 pt.)** Fifth, return three variables, the tuple `acc`, the 3D tensor $a$ and $\\hat{y}$\n",
        "\n",
        "Let's remember that to implement the function rnn_forward, we have access to the following variables and parameters:\n",
        "* `x` - This is the input data for all time steps, a pytorch tensor of shape $(n_x, m, T_x)$\n",
        "* `a0` - The initial hidden state at timestep \"0\", a pytorch tensor of shape $(n_a, m)$\n",
        "* `parameters` - This is a python dictionary. It consists of five keys:\n",
        "* - Wax - The weight matrix multiplying the input $x$ with shape $(n_a, n_x)$\n",
        "* - Waa - The meight matrix multiplying the hidden state $a$ with shape $(n_a, n_a)$\n",
        "* - Wya - The weight matrix for the hidden-state to the output with shape $(n_y, n_a)$\n",
        "* - ba -  Represents the bias with shape $(n_a, 1)$\n",
        "* - by -  Similarly, this is the bias for the hidden-state to the output. It has a shape $(n_y, 1)$\n",
        "    \n",
        "The function must return the following variables:\n",
        "* `a` - This is the next hidden state for all time steps with shape $(n_a, m,T_x)$\n",
        "* `y_pred` - It is the prediction for all time steps with shape $(n_y, m, T_x)$\n",
        "* `acc` - A tuple of values required for the backward pass. The tuple must contain (list of accumulators, x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ZfOhB_sN0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_forward(x, a0, parameters):\n",
        "    \n",
        "    list_accumulators = []\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wya\"].shape\n",
        "    \n",
        "    ### WRITE SOLUTION HERE ###\n",
        "    a = np.zeros((n_a, m, T_x))\n",
        "    y_pred = np.zeros((n_y, m, T_x))\n",
        "    a_next = a0\n",
        "    for t in range(0, T_x):\n",
        "      xt = x[:,:,t]\n",
        "      accumulator, a_next, yt_pred = rnn_cell_forward(xt, a_next, parameters)\n",
        "      a[:,:,t] = a_next\n",
        "      y_pred[:,:,t] = yt_pred\n",
        "      list_accumulators.append(accumulator)\n",
        "    acc = (list_accumulators, x)\n",
        "    return acc, a, y_pred\n",
        "\n",
        "\n",
        " \n",
        "        \n",
        "    ### WRITE SOLUTION HERE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zv3lpv-sN0X",
        "colab_type": "text"
      },
      "source": [
        "## Task 3. Random RNN (1 point)\n",
        "Now that you have defined `rnn_cell_forward` and `rnn_forward`, let's test it with our data and inspect the results. This task consists of three exercises:\n",
        "* **(1 pt.)** First, initialize the RNN cell with the `series` provided above with $x$ of shape $(3, 10, 4)$. Remember also to define random values for the hidden layer $a$ and the weights and biases in the dictionary of parameters. Verify the output by printing `y_pred[1]` and the shape of `y_pred`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRSfq6-0sN0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trend(time, slope=0):\n",
        "    return slope * time\n",
        "\n",
        "\n",
        "def seasonal_pattern(season_time):\n",
        "    \n",
        "    return np.where(season_time < 0.4,\n",
        "                    np.cos(season_time * 2 * np.pi),\n",
        "                    1 / np.exp(3 * season_time))\n",
        "\n",
        "\n",
        "def seasonality(time, period, amplitude=1, phase=0):\n",
        "    season_time = ((time + phase) % period) / period\n",
        "    return amplitude * seasonal_pattern(season_time)\n",
        "\n",
        "\n",
        "def noise(time, noise_level=1, seed=None):\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    return rnd.randn(len(time)) * noise_level\n",
        "\n",
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBMfNBwPsN0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Config of creating time series\n",
        "time = np.arange(120, dtype=\"float32\")\n",
        "baseline = 10\n",
        "series = trend(time, 0.1)\n",
        "baseline = 10\n",
        "amplitude = 40\n",
        "slope = 0.05\n",
        "noise_level = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSDzYE-7sN0h",
        "colab_type": "text"
      },
      "source": [
        "We initialize the time series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl5GqoPYsN0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the time series\n",
        "series = baseline + trend(time, slope) + seasonality(time, period=365,\n",
        "                                                     amplitude=amplitude)\n",
        "# Update with noise\n",
        "series += noise(time, noise_level, seed=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1r1YtkusN0m",
        "colab_type": "text"
      },
      "source": [
        "and plot it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ECACrvNsN0n",
        "colab_type": "code",
        "outputId": "2909e65b-1f62-4bb7-d48b-7ca5cabcfe32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "plot_series(time,series)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEHCAYAAABFroqmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rziW-Y2zsN0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### WRITE SOLUTION HERE ###\n",
        "np.random.seed(10)\n",
        "x = series.reshape(3,10,4)\n",
        "a0 = np.random.randn(4,10)\n",
        "dict_parameters = {\"Waa\": np.random.randn(4,4), \"Wax\": np.random.randn(4,3),\n",
        "                   \"Wya\": np.random.randn(3,4), \"ba\": np.random.randn(4,1), \n",
        "                   \"by\": np.random.randn(3,1)}\n",
        "### WRITE SOLUTION HERE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAtasTSrsN0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accumulators, a, y_pred = rnn_forward(x, a0, dict_parameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9paP7stOsN0v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "643c4695-e4d4-434f-b36e-4a32716c54c0"
      },
      "source": [
        "### WRITE SOLUTION HERE ###\n",
        "y_pred[1].shape\n",
        "### WRITE SOLUTION HERE ###"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BCYkse3sN0y",
        "colab_type": "text"
      },
      "source": [
        "# Home Assignment No. 4: Part 2 (Theory)\n",
        "In this part of the homework you are to solve several simple theoretical problems related to time series forecasting algorithms.\n",
        "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
        "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
        "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
        "## $\\LaTeX$ in Jupyter\n",
        "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
        "write **neat, tidy, and well typeset** equations in your notebooks:\n",
        "* to write an **inline** equation use \n",
        "```markdown\n",
        "$ you latex equation here $\n",
        "```\n",
        "* to write an equation, that is **displayed on a separate line** use \n",
        "```markdown\n",
        "$$ you latex equation here $$\n",
        "```\n",
        "* to write a **block of equations** use \n",
        "```markdown\n",
        "\\begin{align}\n",
        "    left-hand-side\n",
        "        &= right-hand-side on line 1\n",
        "        \\\\\n",
        "        &= right-hand-side on line 2\n",
        "        \\\\\n",
        "        &= right-hand-side on the last line\n",
        "\\end{align}\n",
        "```\n",
        "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
        "(`\\\\`) creates a new line.\n",
        "\n",
        "## The ARMA Process\n",
        "The family of ARMA processses (such as ARIMA and SARIMA) plays a major role in times series forecasting. In the first half of the homework, we will look deeper into understanding important elements around ARMA.\n",
        "\n",
        "We start by remembering that for a time series $\\{X_t\\}$ with $E(X^2_t) < \\infty$, its mean function is defined by $\\mu_X(t) = E(X_t)$ and the sample mean, for the $x_1, \\ldots, x_n$ observations by $\\bar{x} = \\frac{1}{n} \\sum^n_{t=1} x_t$. \n",
        "\n",
        "Similarly, its covariance function is \n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\gamma_X(r,s) = Cov(X_r, X_s) \\\\ \n",
        "= E[(X_r - \\mu_X(r))(X_s - \\mu_X(s))]\n",
        "\\end{aligned}\n",
        "$$ \n",
        "for all integers $r$ and $s$. \n",
        "\n",
        "We know that for $\\{X_t\\}$ to be (weakly) stationary, it must fulfill two conditions. First, $\\mu_X(t)$ must be independent of $t$, and $\\gamma_X(t+h, t)$ must be also independent of $t$ for each h.\n",
        "\n",
        "In the seminar, we applied the autocovariance and the autocorrelation functions to a dataset without deeper discussing their definition. Now, we look deeper into them.\n",
        "\n",
        "#### Autocovariance function\n",
        "The autocovariance function or ACVF of $\\{X_t\\}$ at lag $h$ is $\\gamma X(h) = Cov(X_{t+h}, X_t)$.\n",
        "\n",
        "For the observations $x_1, \\ldots, x_n$, the sample autocovariance function can be similarly defined by\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\gamma} (h) := n^{-1} \\sum^{n - |h|}_{t=1} (x_{t+|h|} - \\bar{x})(x_t - \\bar{x})\n",
        "\\end{aligned}\n",
        "$$ \n",
        "for $-n < h < n$\n",
        "\n",
        "#### Autocorrelation function\n",
        "The autocorrelation function or ACF makes use of ACVF as following $\\rho_X(h) = \\frac{\\gamma X(h)}{\\gamma X(0)} = Cor(X_{t+h}, X_t)$.\n",
        "\n",
        "Similarly, we can write the sample autocorrelation function as \n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\rho} (h) = \\frac{\\hat{\\gamma}(h)}{\\hat{\\gamma}(0)}\n",
        "\\end{aligned}\n",
        "$$ \n",
        "for $-n < h < n$\n",
        "\n",
        "Let's also remember a useful property of covariances, the linear property. It means that if that if $EX^2 < \\infty ,EY^2 < \\infty,EZ^2 < \\infty$ and $a$,$b$,and $c$ are any real constants, then, we can do $Cov(aX + bY + c, Z) = a Cov(X, Z) + b Cov(Y, Z)$.\n",
        "\n",
        "One of the cornerstones of classic forecasting is the Moving Average Process $MA(q)$. It can be defined, if the time series $\\{X_t\\}$ is $Xt = Z_t + \\theta_{1}Z_{t1} +\\ldots+ \\theta_{q}Z_{tq}$, where $\\{Z_t\\} \\sim WN(0,\\sigma^2)$, and $\\theta_{1},\\ldots,\\theta_{q}$ are constants, and $WN$, White Noise, being a sequence of uncorrelated random variables, each with zero mean and variance $\\sigma^2$ defined by $\\sigma^2 = E(X  \\mu)^2$. \n",
        "\n",
        "Let's put this into practice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPJNkydysN00",
        "colab_type": "text"
      },
      "source": [
        "## Task 1. MA(2) (1+1+1=3 points)\n",
        "Let's define a MA(2) with a white noise function of mean 0 and variance 1. Please do the following (**explicitly** outline all your computations):\n",
        "* **(1 pt.)** For $\\theta = 0.9$, derive the formula for the autocovariance and autocorrelation functions for this process.\n",
        "* **(1 pt.)** For $\\theta = 0.9$, compute the variance of the sample mean $(X_1 + X_2 + X_3 + X_4)/4$.\n",
        "* **(1 pt.)** If now we change our $\\theta$ to $-0.9$, what happens? What is the effect of a negative covariance? Please compare the result with the previous computation and **provide an explanation**.\n",
        "### Your Solution:\n",
        "\n",
        "Assume that $\\theta_1 = 0$\n",
        "$$ X_t = Z_t +  \\theta Z_{t-2}$$\n",
        "\n",
        "\n",
        "Lets consider different lags $h$:\n",
        "\n",
        "$$ \\gamma(0) = Cov(X_{t}, X_t) = E[X_t^2] = E[(Z_{t} +  \\theta Z_{t-2})^2] = E[Z_{t}^2]  + \\theta^2 E[Z_{t-2}^2] = 1.81 \\sigma^2 $$\n",
        "\n",
        "$$ \\gamma(1) =  E[(Z_{t} +  \\theta Z_{t-2})(Z_{t+1} +  \\theta Z_{t-1})] = E[(Z_{t} Z_{t+1}+  \\theta Z_{t-2}Z_{t+1}+Z_{t}\\theta Z_{t-1}+\\theta^2Z_{t-1}Z_{t-2}] =  0 \\space(average\\space of\\space white\\space noise\\space 0)$$\n",
        "\n",
        "$$ \\gamma(2) =  E[(Z_{t+2} +  \\theta Z_{t})(Z_{t} +  \\theta Z_{t-2})]= \\theta E[Z_{t}^2] = 0.9 \\sigma^2 $$\n",
        "\n",
        "$$ \\gamma(3) =  E[(Z_{t+3} +  \\theta Z_{t+1})(Z_{t} +  \\theta Z_{t-2})]= 0 $$\n",
        "\n",
        "The autocovariance is:\n",
        "$$ \\hat{\\gamma}(h) = \\left\\{{\\begin{matrix} \n",
        "\\left|h\\right|  = 0, \\gamma=1.81\\\\\n",
        "\\left|h\\right|  = 1, \\gamma=0\\\\\n",
        "\\left|h\\right|  = 2, \\gamma=0.9\\\\\n",
        "\\left|h\\right|  > 2, \\gamma=0\\\\ \\end{matrix}}\\right.$$\n",
        "\n",
        "$\\hat{\\rho} (h) ={\\hat{\\gamma}(h)}/{\\hat{\\gamma}(0)}$ is the autocorrelation:\n",
        "\n",
        "$$ \\hat{\\rho}(h) = \\left\\{{\\begin{matrix} \n",
        "\\left|h\\right|  = 0, \\rho=1\\\\\n",
        "\\left|h\\right|  = 1, \\rho=0\\\\\n",
        "\\left|h\\right|  = 2, \\rho=0.49\\\\\n",
        "\\left|h\\right|  > 2, \\rho =0 \\\\ \\end{matrix}}\\right.$$\n",
        "\n",
        "The variance of the sample mean is the sum of variences of the elements devided by the squared number of elements:\n",
        "$$D[\\frac {(X_1 + X_2 + X_3 + X_4)}{4}] = \\frac{(E[X_1^2] + E[X_2^2] + E[X_3^2] + E[X_4^2])}{16} = \\frac{4(1+\\theta^2)}{16} =\\frac{4(1+0.9^2)}{16} = 0.453 $$\n",
        "\n",
        "For $\\theta = -0.9$ there will be only a change where $\\theta$ is not squared. Namely auticovarience and autocorrelation:\n",
        "\n",
        "$$ \\hat{\\gamma}(h) = \\left\\{{\\begin{matrix} \n",
        "\\left|h\\right|  = 0, \\gamma=1.81\\\\\n",
        "\\left|h\\right|  = 1, \\gamma=0\\\\\n",
        "\\left|h\\right|  = 2, \\gamma=-0.9\\\\\n",
        "\\left|h\\right|  > 2, \\gamma=0\\\\ \\end{matrix}}\\right.$$\n",
        "\n",
        "$$ \\hat{\\rho} = \\left\\{{\\begin{matrix} \n",
        "\\left|h\\right|  = 0, \\rho=1\\\\\n",
        "\\left|h\\right|  = 1, \\rho=0\\\\\n",
        "\\left|h\\right|  = 2, \\rho=-0.49\\\\\n",
        "\\left|h\\right|  > 2, \\rho =0 \\\\ \\end{matrix}}\\right.$$\n",
        "\n",
        "Since the covarience is a measure of joint variability, negative covarience impliese that with the change of one variable in one direction, another varible will change the opposite direction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCgtw2zSsN02",
        "colab_type": "text"
      },
      "source": [
        "*** Please type here your solution ***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3pEUw0CsN03",
        "colab_type": "text"
      },
      "source": [
        "## Task 2. Stationary processes (1+1=2 points)\n",
        "Assume that $\\{Y_t\\}$ represents a sequence with zero mean and variance $\\sigma^2$, let $a$ and $b$ are two constants. Are the following processes stationary? If they are, what would be their respective autocovariance and mean? \n",
        "* **(1 pt.)** $b + aY_0$\n",
        "* **(1 pt.)** $Y_{t}Y_{t-1}$\n",
        "\n",
        "Explicitly clarify your answer and please provide all necessary steps behind your reasoning.\n",
        "### Your Solution:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvPCHJnvsN05",
        "colab_type": "text"
      },
      "source": [
        "*** Please type here your solution ***\n",
        "\n",
        "We have to prove, that these processes fit stationary conditions\n",
        "1. $b + aY_0$\n",
        "$$\n",
        "E[(b + aY_0)] = E[b]+ E[aY_0] = b\n",
        "$$\n",
        "The mean is the constant value\n",
        "$$\n",
        "\\gamma(h) = Cov(b + aY_{o+h}, b + aY_0) = Cov(aY_{o+h}, aY_0) = a^2Cov(Y_{o+h}, Y_0)\n",
        "$$\n",
        "This autocovariance is equal to $a^2\\sigma^2$ in case h = 0 and $a^2\\mu = 0$ otherwise. That means both mean and variance are not time-dependent and process satisfies weak stationary conditions.\n",
        "1. $Y_{t}Y_{t-1}$\n",
        "The mean of sequence is zero from the given data and thus constant:\n",
        "$$\n",
        "E[Y_{t}Y_{t-1}] = 0\n",
        "$$\n",
        "Autocovariance:\n",
        "$$\n",
        "\\gamma(h) = Cov(Y_{t+h}Y_{t-1+h}, Y_{t}Y_{t-1})=E[(Y_{t+h}Y_{t-1+h}-\\mu_h)(Y_{t}Y_{t-1}-\\mu_0)]=E[(Y_{t+h}Y_{t-1+h}-E[(Y_{t+h}Y_{t-1+h}])(Y_{t}Y_{t-1}-E[Y_{t}Y_{t-1}]]= E(Y_{t+h}Y_{t-1+h} Y_{t}Y_{t-1})\n",
        "$$\n",
        "It equals to $\\sigma^4$ in case $h=0$ and 0 otherwise, thus the process also satisfies weak stationary conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2G_GiPGsN06",
        "colab_type": "text"
      },
      "source": [
        "## Task 3. Questionable processes (1 point)\n",
        "Assume that we have two moving average processes MA(1) and $0 < |\\theta| < 1$. The first process is defined by $Q_t + \\frac{1}{\\theta}Q_{t-1}$. It has a White Noise with variance $\\sigma^2\\theta^2$ and zero mean. The second one is $S_t + \\theta S_{t1}$ with a White Noise with zero mean and variance $\\sigma^2$.\n",
        "\n",
        "Do they have the same autocovariance functions? Answer this question by deriving exact \n",
        "analytic formulas for autocovariation functions.\n",
        "### Your solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EL0yvMAsN07",
        "colab_type": "text"
      },
      "source": [
        "*** Please type here your solution ***\n",
        "\n",
        "Lets find the covarience for different lags $h$ for the processes:\n",
        "\n",
        "\n",
        "$$ \\gamma_1(0) = E[(Q_t + \\frac {1}{\\theta} Q_{t-1})^2] = E[Q_t^2] + E[\\frac{Q_{t-1}^2}{\\theta^2}] = \\theta^2 \\sigma^2 + \\sigma^2$$\n",
        "$$ \\gamma_2(0) = E[(S_t + \\theta S_{t-1})^2] = E[S_t^2]  + \\theta^2 E[S_t^2] = \\sigma^2 + \\theta^2 \\sigma^2$$\n",
        "\n",
        "$$ \\gamma_1(1) =  E[(Q_{t+1} + \\frac {1}{\\theta} Q_{t})(Q_{t} + \\frac {1}{\\theta} Q_{t-1})] = E[\\frac{Q_{t}^2}{\\theta}] = \\theta \\sigma^2 $$\n",
        "$$ \\gamma_2(1) =  E[(S_{t+1} + \\theta S_{t})(S_t + \\theta S_{t-1})] = \\theta E[S_t^2] =  \\theta \\sigma^2$$\n",
        "\n",
        "$$ \\gamma_1(2) =  E[(Q_{t+2} + \\frac {1}{\\theta} Q_{t+1})(Q_{t} + \\frac {1}{\\theta} Q_{t-1})] = 0 $$\n",
        "$$ \\gamma_1(2) =  E[(S_{t+2} + \\theta S_{t+1})(S_t + \\theta S_{t-1})] = 0 $$\n",
        "\n",
        "For all values, covering all possible cases, the value of autocovarience is the same.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsjTQAWYsN08",
        "colab_type": "text"
      },
      "source": [
        "## ARMA\n",
        "Let's start by defining the ARMA(p, q) process for a time series ${Xt}$. This is the case if the time series is stationary and for every $t$, it satisfies\n",
        "$$\n",
        "\\begin{aligned}\n",
        "X_t  \\phi X_{t1} - \\ldots -  \\phi_p X_{tp} = Z_{t} + \\theta Z_{t1} + \\ldots + \\theta_q Z_{t-q},\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where ${Z_t} \\sim WN(0, \\sigma^2)$ In addition, the polynomials $1+ \\theta_{1}z + \\ldots + \\theta_{q}z^q$ and $1  \\phi_{1}z  \\ldots  \\phi_{p}z^p$ have no common factors.\n",
        "\n",
        "\n",
        "\n",
        "If we define $B$ by the Backwards Shift Operator $BX_{t} = X_{t1}$ or more generally $B^j X_t = X_{tj}, B^j Z_t = Z_{tj}, j = 0, \\pm 1, \\ldots$, we can write this more cleanly as \n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\phi(B)X_t = \\theta(B)Z_t,\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\phi()$ and $\\theta()$ are the p-th and q-th-degree polynomials: $\\phi(z) = 1  \\phi_1 z  \\ldots  \\phi_{p} z^p$ and\n",
        "$\\theta(z) = 1 + \\theta_1 z + \\ldots + \\theta_{q}z^q$.\n",
        "### The Partial Autocorrelation Function\n",
        "The partial autocorrelation function (PACF) of an ARMA process $\\{X_t\\}$ is the function $\\alpha()$ defined by the equations\n",
        "\n",
        "$$\\alpha(0) = 1 \\qquad \\text{and}\\qquad \\alpha(h) = \\phi_{hh}\\text{, } h \\geq 1,$$\n",
        "\n",
        "where $\\phi_{hh}$ is the last component of $\\phi_{h} = \\Gamma^{-1}_h \\gamma_h$. Here, $\\Gamma_h = [\\gamma(i - j)]^h_{i,j = 1}$ and $\\gamma_h = [\\gamma(1), \\gamma(2), \\ldots, \\gamma(h)]$\n",
        "\n",
        "Similarly, the sample PACF is the funciton $\\hat{\\alpha}()$, where \n",
        "\n",
        "$$\\hat{\\alpha}(0) = 1\\qquad \\text{and} \\qquad \\hat{\\alpha}(h) = \\hat{\\phi}_{hh}\\text{, } h \\geq 1,$$\n",
        "where $\\hat{\\phi}_{hh}$ is the last component of $\\hat{\\phi}_{h} = \\Gamma^{-1}_h \\hat{\\gamma}_h$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jruj4ljsN08",
        "colab_type": "text"
      },
      "source": [
        "## Task 4. ARMA (3 points)\n",
        "Assuming that the best linear predictor of $X_{h+1}$ can be estimated as $\\hat{X}_{h+1} = \\sum^h_{i=1} \\phi_{hi} X_{h+1-i}$, compute the PACF $\\alpha(2)$ of the moving average process MA(1) defined by $X_t = Q_t + \\theta Q_{t-1}$, where $\\{Q_t\\}$ is a White Noise with zero mean and variance $\\sigma^2$.\n",
        "### Your solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75l1RAXtsN09",
        "colab_type": "text"
      },
      "source": [
        "*** Please type here your solution ***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKbYFJ2bsN0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}