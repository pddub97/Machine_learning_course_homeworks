{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "ML2020HW01-part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHu3HH-jkAMU",
        "colab_type": "text"
      },
      "source": [
        "# Home Assignment No. 1: Part 2 (Theory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIKo8G6ekAMX",
        "colab_type": "text"
      },
      "source": [
        "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
        "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
        "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
        "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
        "## $\\LaTeX$ in Jupyter\n",
        "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
        "write **neat, tidy, and well typeset** equations in your notebooks:\n",
        "* to write an **inline** equation use \n",
        "```markdown\n",
        "$ you latex equation here $\n",
        "```\n",
        "* to write an equation, that is **displayed on a separate line** use \n",
        "```markdown\n",
        "$$ you latex equation here $$\n",
        "```\n",
        "* to write a **block of equations** use \n",
        "```markdown\n",
        "\\begin{align}\n",
        "    left-hand-side\n",
        "        &= right-hand-side on line 1\n",
        "        \\\\\n",
        "        &= right-hand-side on line 2\n",
        "        \\\\\n",
        "        &= right-hand-side on the last line\n",
        "\\end{align}\n",
        "```\n",
        "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
        "(`\\\\`) creates a new line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJDSqti6kAMY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Task 1. Linear Regression (1 point)\n",
        "Let us consider the problem of linear regression for 2D data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{2+ 1}$. Let us have $l_{\\infty}$ regularization penalty, i.e. the optimization problem is\n",
        "$$\n",
        "||Xw - y||_2^2 + \\lambda||w||_{\\infty} \\rightarrow \\min_{\\boldsymbol{w}}\n",
        "$$\n",
        "\n",
        "Show that this problem is equal to Lasso regression problem with feature matrix $Z = XA \\in \\mathbb{R}^{n \\times 2}$ for a certain $2 \\times 2$ matrix $A$ and the same target $y$.\n",
        "### Your solution:\n",
        "Lasso regression problem\n",
        "$$\n",
        "||Xw' - y||_2^2 + \\lambda||w'||_1 \\rightarrow \\min_{\\boldsymbol{w'}}\n",
        "$$\n",
        "And also denote, that:\n",
        "$$\n",
        "w = Aw'\n",
        "$$\n",
        "Two optimization problems are equivalent if there exist a transformation rule, by which it is possible to transform feasible solution of one problem to the feasible solution of another problem. From the level lines for the maximum and $l_1$ norms we can conclude, that we can get $l_\\infty$ norm by rotation 45 degrees and scaling of $l_1$ norm.\n",
        "Let's suppose matrix $A$ is a rotation matrix, multiplied by the coefficient $\\sqrt2$:\n",
        "$$\n",
        "A=\n",
        "\\begin{pmatrix}\n",
        "\\sqrt2 cos(45^{\\circ}) & \\sqrt2 (-sin(45^{\\circ}))\\\\\n",
        "\\sqrt2 sin(45^{\\circ}) & \\sqrt2 cos(45^{\\circ})\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "1 & -1\\\\\n",
        "1 & 1\n",
        "\\end{pmatrix}\n",
        "$$ \n",
        "then:\n",
        "$$\n",
        "A{\\boldsymbol{w'}}=\n",
        "\\begin{pmatrix}\n",
        "1 & -1\\\\\n",
        "1 & 1\n",
        "\\end{pmatrix}\n",
        "{\\cdot}\n",
        "\\begin{pmatrix}\n",
        "w' \\\\\n",
        "b'\n",
        "\\end{pmatrix}=\n",
        "\\begin{pmatrix}\n",
        "w'-b'\\\\\n",
        "w'+b'\n",
        "\\end{pmatrix}=\n",
        "\\begin{pmatrix}\n",
        "w \\\\\n",
        "b\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "Norm $l_1$ implies finding the sum of vector's elements absolute values:\n",
        "$$\n",
        "|w'|+|b'|\n",
        "$$\n",
        "Maximum norm implies finding the greatest module among vector elements:\n",
        "$$\n",
        "\\operatorname{argmax}(|w'+b'|, |w'-b'|)\n",
        "$$\n",
        "Wheights are always positive numbers, while an intersept could be positive or negative, in case it is negative, the right term becomes larger, in case it is positive, the left term is larger, but anyway, we will get the sum $|w'|+|b'|$, as well, as in norm $l_1$. Thus, by multiplying the weights vector of the Lasso regression by the rotation matrix we can get the same problem, but stated as Linear regression with norm $l_{\\infty}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ0E40eRkAMZ",
        "colab_type": "text"
      },
      "source": [
        "## Task 2. Probit Regression (1 point)\n",
        "Let us consider the Probit regression model for a binary classification problem. It is a linear model analogous to the Logistic Regression. For a feature vector $x \\in \\mathbb{R}^d$ the probability for label $y$ being equal to 1 is given by\n",
        "$$P(y=1|x) = \\Phi(x^Tw + b).$$ \n",
        "Here $\\Phi(x)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt$ is the Cumulative Density Function for the **standard normal distribution**; values $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are learnable parameters of the probit regression model.\n",
        "\n",
        "Write down the optimization problem for training probit  probit regression **without L2-regularization** and show that it does not have optimum in the case when the training set is **lineary separable**.\n",
        "\n",
        "### Your solution:\n",
        "Parameters $w$ for the $\\Phi(x^Tw + b)$ are usually estimated by the maximum likelihood method. Likelyhood for a single observation\n",
        "$\n",
        "(y_i, x_i)$ is\n",
        "$$\n",
        "\\mathcal{L}(w; y_i, x_i) = \\Phi(x_i^Tw+b)^{y_i} [1-\\Phi(x_i^Tw+b)]^{(1-y_i)}\n",
        "$$\n",
        "Observations are independant and identically distributed, from which we can write joint likelihood for the whole sample simply as a product of likelihoods of all observations:\n",
        "$$\n",
        "\\mathcal{L}(w; Y, X) = \\prod_{i=1}^n \\left( \\Phi(x_i^Tw+b)^{y_i} [1-\\Phi(x_i^Tw+b)]^{(1-y_i)} \\right)\n",
        "$$\n",
        "Let's transorm the likelihood function to the log-likelyhood:\n",
        "$$\n",
        "\\ln\\mathcal{L}(w; Y, X) = \\sum_{i=1}^n \\bigg( y_i\\ln\\Phi(x_i^Tw+b) + (1-y_i)\\ln\\!\\big(1-\\Phi(x_i^Tw+b)\\big) \\bigg)\n",
        "$$\n",
        "If we want to minimize this log-likelihood function, the hyperplane, corresponding to the optimal solution $w^*$ can be scaled without failing to be a solution, but the function $\\Phi(x_i^Tw+b)$ is a probability in its nature and is bounded by 1.Let's set  the scaling coefficient $k\\rightarrow\\infty$. Then the second term of the sum is multiplied by $\\ln0\\rightarrow-\\infty$ and finally the whole limit $\\lim_{k \\rightarrow +\\infty} \\ln\\mathcal{L}(kw^*; Y, X)\\rightarrow -\\infty$ which means the minimum is $-\\infty$ and it doesn't actually exist.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsGDWUeVkAMa",
        "colab_type": "text"
      },
      "source": [
        "## Task 3. Multiclass Bayesian Naive Classifier (1 point)\n",
        "Let us consider multiclass classification problem with classes $C_1, \\ldots, C_K$. Assume that all $d$ features $x_1, \\dots,x_d$ are binary, i.e. $x_{i}\\in\\{0,1\\}$ for $i=1,2\\dots,d$. Show that the decision rule of a Bayesian Naive Classifier can be represented as $\\arg \\max$ of linear functions of the input. \n",
        "### Your solution:\n",
        "Naive Bayes Classifier maximizes posterior probability of observing class $C$ under a certain set of features $x_1, \\dots,x_d$:\n",
        "$$\\hat{y} = \\underset{C=C_1\\dots,C_K}{\\operatorname{argmax}} p(C) \\prod\\limits_{i=1}^d p(x_i| C) \n",
        "$$\n",
        "To linearize this expression let's apply logarithm function, since we know that logaritm of product is a sum of logarithms of the corresponding multipliers:\n",
        "$$\n",
        "\\big[\\text{apply} \\log\\big] = \\underset{C=C_1\\dots,C_K}{\\operatorname{argmax}} \\big(\\log p(C) + \\sum\\limits_{i=1}^d \\log p(x_i|C)\\big) = \\underset{C=C_1\\dots,C_K}{\\operatorname{argmax}} \\big(\\log p(C) + \\sum\\limits_{i=1}^d \\left( x_i\\log p(x_i = 1|C) + (1 - x_i) \\log p(x_i = 0|C) \\right)\\big) = \\underset{C=C_1\\dots,C_K}{\\operatorname{argmax}} \\big( \\underbrace{\\log p(C) + \\sum\\limits_{i=1}^d \\log p(x_i=0|C)}_{b^{C}} + \\sum\\limits_{i=1}^d x_i \\underbrace{\\frac{\\log p(x_i=1|C)}{p(x_i=0|C)}}_{w_i^{C}} \\big) = \\underset{C=C_1\\dots,C_K}{\\operatorname{argmax}} b^{C} + \\sum\\limits_{i=1}^d x_i w_i^{C}\n",
        "$$\n",
        "Both $b^{C}$ and $w_i^{C}$ are $x_i$-independant, so we can conclude\n",
        "that the argument od argmax is linear function of an input.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQOKJ2glkAMb",
        "colab_type": "text"
      },
      "source": [
        "## Task 4. Nearest Neighbors (1 point)\n",
        "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
        "\n",
        "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
        "$$L_{k}OCV=\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg),$$\n",
        "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
        "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
        "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
        "### Your solution:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frO_xdQ7kAMc",
        "colab_type": "text"
      },
      "source": [
        "## Task 5. Bootstrap (1 point)\n",
        "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability that object $x_{i}$ is not present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
        "### Your solution:\n",
        "The probability of taking one specific object from the set of $n$ objects is $\\frac{1}{n}$. The probability to take another object is $1-\\frac{1}{n}$. The probability to not take neede object from the second time is a product of probability to take a different object for the first time multiplied by the probability to take different object one more time $(1-\\frac{1}{n})(1-\\frac{1}{n})$ Thus the probability of some object $x_i$ to not get into sample subsample $\\hat{X}^{n}$ is simply $(1-\\frac{1}{n})^n$. With the $n\\rightarrow\\infty$ we get:\n",
        "$$\n",
        "\\lim_{n \\rightarrow +\\infty} \\left( 1 - \\frac{1}{n} \\right)^n = [ n \\rightarrow -n] = \\lim_{n \\rightarrow -\\infty} \\frac{1}{\\left( 1 + \\frac{1}{n} \\right)^n} = \\frac{1}{e}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37xfBH8VkAMd",
        "colab_type": "text"
      },
      "source": [
        "## Task 6. Decision Tree Leaves (1+1=2 points)\n",
        "\n",
        "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample.\n",
        "\n",
        "* For a classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$) and zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$, find the optimal prediction in the leaf.\n",
        "\n",
        "* For a regression tree ($y_{i}\\in\\mathbb{R}$) and squared percentage error loss $L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$ find the optimal prediction in the leaf.\n",
        "### Your solution:\n",
        "* Zero-one loss function gives a zero in case of the correct guess of the label and 1 in case of an incorrect guess. Such a function is not differentiable, so we cannot find a minimum by zero derivative, but it is obvious, that it is most probable, that the most frequent class in a sample will give the highest probability of being correct while guessing. It means, that all we can do to optimize out leaf prediction is to assign $\\hat{y}$ equal to this class.\n",
        "\n",
        "* Finding the optimal prediction $\\hat{y}$ means the following optimization problem:\n",
        "$$  \\underset{y \\in \\mathbb{R}}{\\operatorname{min}} \\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{(y_i - \\hat{y})^2}{y_i^2}.$$\n",
        "To obtain optimum, let's compute derivative of the function above with respect to $\\hat{y}$:\n",
        "$$ -\\frac{2}{n} \\sum\\limits_{i=1}^n \\frac{(y_i - \\hat{y})}{y_i^2} = 0 $$\n",
        "Then $\\hat{y}$:\n",
        "$$ \\hat{y} = \\frac{y_1 y_2^2 \\ldots y_n^2 + \\ldots + y_n y_1^2 \\ldots y_{n-1}^2}{y_2^2 \\ldots y_n^2 + \\ldots + y_1^2 \\ldots y_{n-1}^2} = \\frac{\\sum\\limits_{i=1}^n y_i \\prod\\limits_{k=1, k \\neq i}^n y_k^2}{\\sum\\limits_{i=1}^n \\prod\\limits_{k=1, k \\neq i}^n y_k^2}.$$\n",
        "$\\prod\\limits_{k=1, k \\neq i}^n y_k^2$ are the constant values, that makes the ratio of sums above a weighted arithmetic mean.\n",
        "But the found value is only a point of zero gradient. It could be either maximum or minimum. For this point to be a minimum we have to check, that the second derivative is positive: $$ \\left( -\\frac{2}{n} \\sum\\limits_{i=1}^n \\frac{(y_i - \\hat{y})}{y_i^2} \\right)' = \\frac{2}{n} \\sum\\limits_{i=1}^n \\frac{1}{y_i^2}$$\n",
        "The point found is a minimum point, since the second derivative can be only a positive number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX5ZeU6YkAMe",
        "colab_type": "text"
      },
      "source": [
        "## Task 7. Classification (1 point)\n",
        "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
        "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
        "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$.\n",
        "### Your solution:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZQeRaJgkAMf",
        "colab_type": "text"
      },
      "source": [
        "## Task 8. Kernels (1+1=2 points)\n",
        "Kernel $K(x,y)$ corresponds to dot product of feature maps $\\varphi$ and therefore $K(x,y) = \\langle \\varphi(x), \\varphi(y) \\rangle$. Derive functions $\\varphi$ for the following kernels:\n",
        "* $K(x,y)=\\langle x, y \\rangle ^ d$;\n",
        "* $K(x,y)= \\left(c + \\langle x, y \\rangle \\right)^ d$  with $c\\geq 0$;\n",
        "\n",
        "### Your solution:\n",
        "* Dot product is a sum of element-wise multiplications of the corresponding elements from vectors of the same dimension. Thus we can apply the multinomial theorem, which expands power of sum by means of power of terms in that sum.\n",
        "$$\n",
        "K(x,y)=\\langle x, y \\rangle ^ d=\\left( \\sum\\limits_{i=1}^n x_i y_i \\right)^d= \\sum\\limits_{k_1+\\ldots+k_n=d} \\binom{d}{k_1,\\ldots,k_n} \\prod\\limits_{i=1}^n (x_i y_i)^{k_i}\n",
        "$$\n",
        "Where \n",
        "$$\\binom{d}{k_1,\\ldots,k_n} = \\frac{n!}{k_1!k_2!\\ldots k_d!}\n",
        "$$ \n",
        "is a multinomial coefficient and thus, a scalar value. Let's split a product onto two different products to get a product of functions \n",
        "$$\n",
        "\\sum\\limits_{k_1+\\ldots+k_n=d} \\sqrt{\\binom{d}{k_1,\\ldots,k_n}} \\prod\\limits_{i=1}^n x_i^{k_i} \\sqrt{\\binom{d}{k_1,\\ldots,k_n}} \\prod\\limits_{i=1}^n y_i^{k_i} = \\varphi(x)^T \\varphi(y) = \\langle \\varphi(x), \\varphi(y) \\rangle\n",
        "$$\n",
        "So, the feature map w.r.t x:\n",
        "$$\n",
        "\\varphi(x) = \n",
        "\\begin{pmatrix}\n",
        "\\sqrt{\\binom{d}{k_1,\\ldots,k_n}} \\prod\\limits_{i=1}^n x_i^{k_i}\n",
        "\\\\\n",
        "\\ldots\n",
        " \\\\\n",
        "\\ldots\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "This vector has a length equal to the amount of coefficient combinations\n",
        "* Now there is a constant inside the power.\n",
        "$$\n",
        "K(x,y)=\\left(c + \\langle x, y \\rangle \\right)^ d=\\left( c+\\sum\\limits_{i=1}^n x_i y_i \\right)^d=\\sum\\limits_{k_1+\\ldots+k_{n+1}=d} \\binom{d}{k_1,\\ldots,k_{n+1}} c^{k_1} \\prod\\limits_{i=2}^{n+1} (x_{i-1} y_{i-1})^{k_i} = \\sum\\limits_{k_0+\\ldots+k_{n+1}=d} \\sqrt{\\binom{d}{k_1,\\ldots,k_{n+1}} c^{k_1}} \\prod\\limits_{i=2}^{n+1} x_{i-1}^{k_i} \\sqrt{\\binom{d}{k_1,\\ldots,k_{n+1}} c^{k_1}} \\prod\\limits_{i=2}^{n+1} y_{i-1}^{k_i} = \\langle \\varphi(x), \\varphi(y) \\rangle ,\n",
        " $$\n",
        "As we can see, the constant went under the square root sign\n",
        "$$\n",
        "\\varphi(x) = \n",
        "\\begin{pmatrix}\n",
        "\\sqrt{\\binom{d}{k_1,\\ldots,k_{n+1}} c^{k_1}} \\prod\\limits_{i=2}^{n+1} x_{i-1}^{k_i}\n",
        "\\\\\n",
        "\\ldots\n",
        " \\\\\n",
        "\\ldots\n",
        "\\end{pmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx3qEtfZkAMg",
        "colab_type": "text"
      },
      "source": [
        "## Task 9. Kernel Methods (1 point)\n",
        "Prove that Gaussian Kernel $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite. You do not need to prove that the linear kernel is positive definite.\n",
        "### Your solution:\n",
        "$$K(x,x') = \\exp(-||x-x'||^2) =\\exp((x-x')^T(x-x')) = \\exp(x^Tx - x^Tx' - (x')^Tx + (x')^Tx') = \\\\exp(||x||^2 - 2x^Tx' + ||x'||^2)= \\exp(-||x||^2)\\exp(2x^Tx')\\exp(-||x'||^2) \\space (1)$$\n",
        "It is given, that the linear kernel is positive definite.\n",
        "$$\n",
        "K(x,x') = \\langle x,x' \\rangle = x^Tx'\n",
        "$$\n",
        "Multiplication by scalar doesn't change positive-definiteness (kernel's property), so $2 \\langle x,x' \\rangle$ is also positive definite. Exponent of the positive definite kernel also positive definite (Another kernel's property). So $\\exp(2x^Tx')$ is positive definite. There is also another kernel's property which states, that if $K(x,x') $ is positive definite, then $f(x)K(x,x')f(x')$ is also positive definite. Let't define $f(x)=\\exp(-||x||^2)$. Then we can rewrite (1) as $f(x)K(x,x')f(x')$ which is exactly fits the third kernel's property and it means, that $K(x,x') = \\exp(-||x||^2)\\exp(2x^Tx')\\exp(-||x'||^2)$ is a positive definite kernel. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqm4NKymkAMg",
        "colab_type": "text"
      },
      "source": [
        "## Task 10. Support Vector Machine (1 point)\n",
        "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
        "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
        "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
        "### Your solution:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5w4FcJNkAMh",
        "colab_type": "text"
      },
      "source": [
        "The end."
      ]
    }
  ]
}